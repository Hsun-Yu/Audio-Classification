{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification\n",
    "This notebook will build a sample DNN to classify different class of audios\n",
    "\n",
    "## Get features\n",
    "In this notebook, I will not use all of features because it will be a 2D image(the input of DNN will be too many). So I will do average for all amplitude of same frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Failed to connect to remote Jupyter notebook.\r\nCheck that the Jupyter Server URI setting has a valid running server specified.\r\nhttp://114.34.48.196:8888/\r\nTypeError: request to http://114.34.48.196:8888/api/contents/?1583486875079 failed, reason: connect ECONNREFUSED 114.34.48.196:8888",
     "output_type": "error",
     "traceback": [
      "Error: Failed to connect to remote Jupyter notebook.",
      "Check that the Jupyter Server URI setting has a valid running server specified.",
      "http://114.34.48.196:8888/",
      "TypeError: request to http://114.34.48.196:8888/api/contents/?1583486875079 failed, reason: connect ECONNREFUSED 114.34.48.196:8888",
      "at /home/hsunyu/.vscode-server/extensions/ms-python.python-2020.2.64397/out/client/extension.js:1:817679",
      "at processTicksAndRejections (internal/process/task_queues.js:89:5)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=120)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "    return mfccsscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset and get all features of all audios\n",
    "1. Read the csv file\n",
    "\n",
    "loop\n",
    "\n",
    "2. get and save features of the audio of the row\n",
    "\n",
    "3. save the label\n",
    "\n",
    "Look the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Finished feature extraction from  2000  files\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full audio dataset \n",
    "fulldatasetpath = './ESC-50/audio/'\n",
    "\n",
    "metadata = pd.read_csv('./ESC-50/meta/esc50.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),str(row[\"filename\"]))\n",
    "    class_label = row[\"category\"]\n",
    "    data = extract_features(file_name)\n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data and labels\n",
    "I will use sklearn.preprocessing.LabelEncoder to encode the categorical text data into model-understandable numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "Here I will use sklearn.model_selection.train_test_split to split the dataset into training and testing sets. The testing set size will be 20% and we will set a random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DNN model\n",
    "Look the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_shape=(120,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_4 (Dense)              (None, 256)               30976     \n_________________________________________________________________\nactivation_4 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 256)               65792     \n_________________________________________________________________\nactivation_5 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 50)                12850     \n_________________________________________________________________\nactivation_6 (Activation)    (None, 50)                0         \n=================================================================\nTotal params: 109,618\nTrainable params: 109,618\nNon-trainable params: 0\n_________________________________________________________________\nPre-training accuracy: 1.2500%\n"
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "accuracy = 100 * score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "============] - 0s 58us/step - loss: 0.4456 - accuracy: 0.8819 - val_loss: 4.8370 - val_accuracy: 0.4725\n\nEpoch 01902: val_loss did not improve from 2.27184\nEpoch 1903/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.5288 - accuracy: 0.8606 - val_loss: 4.7009 - val_accuracy: 0.4775\n\nEpoch 01903: val_loss did not improve from 2.27184\nEpoch 1904/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.5017 - accuracy: 0.8700 - val_loss: 4.6013 - val_accuracy: 0.4725\n\nEpoch 01904: val_loss did not improve from 2.27184\nEpoch 1905/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4951 - accuracy: 0.8781 - val_loss: 4.6207 - val_accuracy: 0.4875\n\nEpoch 01905: val_loss did not improve from 2.27184\nEpoch 1906/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.4812 - accuracy: 0.8850 - val_loss: 4.7921 - val_accuracy: 0.4825\n\nEpoch 01906: val_loss did not improve from 2.27184\nEpoch 1907/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4664 - accuracy: 0.8894 - val_loss: 4.8155 - val_accuracy: 0.4925\n\nEpoch 01907: val_loss did not improve from 2.27184\nEpoch 1908/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4977 - accuracy: 0.8800 - val_loss: 4.8952 - val_accuracy: 0.4850\n\nEpoch 01908: val_loss did not improve from 2.27184\nEpoch 1909/2000\n1600/1600 [==============================] - 0s 61us/step - loss: 0.4594 - accuracy: 0.8875 - val_loss: 5.0531 - val_accuracy: 0.4675\n\nEpoch 01909: val_loss did not improve from 2.27184\nEpoch 1910/2000\n1600/1600 [==============================] - 0s 63us/step - loss: 0.5027 - accuracy: 0.8769 - val_loss: 4.8133 - val_accuracy: 0.4675\n\nEpoch 01910: val_loss did not improve from 2.27184\nEpoch 1911/2000\n1600/1600 [==============================] - 0s 121us/step - loss: 0.4340 - accuracy: 0.8775 - val_loss: 4.7656 - val_accuracy: 0.4700\n\nEpoch 01911: val_loss did not improve from 2.27184\nEpoch 1912/2000\n1600/1600 [==============================] - 0s 230us/step - loss: 0.5567 - accuracy: 0.8731 - val_loss: 4.8721 - val_accuracy: 0.4700\n\nEpoch 01912: val_loss did not improve from 2.27184\nEpoch 1913/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4337 - accuracy: 0.8769 - val_loss: 5.0058 - val_accuracy: 0.4650\n\nEpoch 01913: val_loss did not improve from 2.27184\nEpoch 1914/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4764 - accuracy: 0.8788 - val_loss: 5.0774 - val_accuracy: 0.4675\n\nEpoch 01914: val_loss did not improve from 2.27184\nEpoch 1915/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4323 - accuracy: 0.8881 - val_loss: 4.9188 - val_accuracy: 0.4650\n\nEpoch 01915: val_loss did not improve from 2.27184\nEpoch 1916/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4863 - accuracy: 0.8775 - val_loss: 4.8582 - val_accuracy: 0.4725\n\nEpoch 01916: val_loss did not improve from 2.27184\nEpoch 1917/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.4743 - accuracy: 0.8856 - val_loss: 4.7862 - val_accuracy: 0.4850\n\nEpoch 01917: val_loss did not improve from 2.27184\nEpoch 1918/2000\n1600/1600 [==============================] - 0s 63us/step - loss: 0.4317 - accuracy: 0.8788 - val_loss: 4.8809 - val_accuracy: 0.4875\n\nEpoch 01918: val_loss did not improve from 2.27184\nEpoch 1919/2000\n1600/1600 [==============================] - 0s 61us/step - loss: 0.4517 - accuracy: 0.8919 - val_loss: 4.8290 - val_accuracy: 0.4850\n\nEpoch 01919: val_loss did not improve from 2.27184\nEpoch 1920/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4653 - accuracy: 0.8731 - val_loss: 4.8634 - val_accuracy: 0.4750\n\nEpoch 01920: val_loss did not improve from 2.27184\nEpoch 1921/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4728 - accuracy: 0.8806 - val_loss: 4.9158 - val_accuracy: 0.4625\n\nEpoch 01921: val_loss did not improve from 2.27184\nEpoch 1922/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.5164 - accuracy: 0.8831 - val_loss: 4.8665 - val_accuracy: 0.4775\n\nEpoch 01922: val_loss did not improve from 2.27184\nEpoch 1923/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4803 - accuracy: 0.8925 - val_loss: 4.7729 - val_accuracy: 0.4875\n\nEpoch 01923: val_loss did not improve from 2.27184\nEpoch 1924/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.5839 - accuracy: 0.8694 - val_loss: 4.6120 - val_accuracy: 0.4725\n\nEpoch 01924: val_loss did not improve from 2.27184\nEpoch 1925/2000\n1600/1600 [==============================] - 0s 72us/step - loss: 0.4920 - accuracy: 0.8725 - val_loss: 4.5425 - val_accuracy: 0.4650\n\nEpoch 01925: val_loss did not improve from 2.27184\nEpoch 1926/2000\n1600/1600 [==============================] - 0s 69us/step - loss: 0.5099 - accuracy: 0.8706 - val_loss: 4.6760 - val_accuracy: 0.4775\n\nEpoch 01926: val_loss did not improve from 2.27184\nEpoch 1927/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.5057 - accuracy: 0.8800 - val_loss: 4.7173 - val_accuracy: 0.4725\n\nEpoch 01927: val_loss did not improve from 2.27184\nEpoch 1928/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.5458 - accuracy: 0.8662 - val_loss: 4.5798 - val_accuracy: 0.4700\n\nEpoch 01928: val_loss did not improve from 2.27184\nEpoch 1929/2000\n1600/1600 [==============================] - 0s 54us/step - loss: 0.4100 - accuracy: 0.8956 - val_loss: 4.5628 - val_accuracy: 0.4875\n\nEpoch 01929: val_loss did not improve from 2.27184\nEpoch 1930/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4770 - accuracy: 0.8844 - val_loss: 4.5655 - val_accuracy: 0.4950\n\nEpoch 01930: val_loss did not improve from 2.27184\nEpoch 1931/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4980 - accuracy: 0.8875 - val_loss: 4.5923 - val_accuracy: 0.4925\n\nEpoch 01931: val_loss did not improve from 2.27184\nEpoch 1932/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.3522 - accuracy: 0.9075 - val_loss: 4.5634 - val_accuracy: 0.4950\n\nEpoch 01932: val_loss did not improve from 2.27184\nEpoch 1933/2000\n1600/1600 [==============================] - 0s 59us/step - loss: 0.4114 - accuracy: 0.8906 - val_loss: 4.6822 - val_accuracy: 0.4875\n\nEpoch 01933: val_loss did not improve from 2.27184\nEpoch 1934/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4499 - accuracy: 0.8875 - val_loss: 4.6736 - val_accuracy: 0.4625\n\nEpoch 01934: val_loss did not improve from 2.27184\nEpoch 1935/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.5042 - accuracy: 0.8756 - val_loss: 4.7205 - val_accuracy: 0.4825\n\nEpoch 01935: val_loss did not improve from 2.27184\nEpoch 1936/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4636 - accuracy: 0.8894 - val_loss: 4.7051 - val_accuracy: 0.4700\n\nEpoch 01936: val_loss did not improve from 2.27184\nEpoch 1937/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.5248 - accuracy: 0.8800 - val_loss: 4.6799 - val_accuracy: 0.4625\n\nEpoch 01937: val_loss did not improve from 2.27184\nEpoch 1938/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.5492 - accuracy: 0.8700 - val_loss: 4.6046 - val_accuracy: 0.4725\n\nEpoch 01938: val_loss did not improve from 2.27184\nEpoch 1939/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.5539 - accuracy: 0.8600 - val_loss: 4.6160 - val_accuracy: 0.4625\n\nEpoch 01939: val_loss did not improve from 2.27184\nEpoch 1940/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.5246 - accuracy: 0.8675 - val_loss: 4.6892 - val_accuracy: 0.4725\n\nEpoch 01940: val_loss did not improve from 2.27184\nEpoch 1941/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4781 - accuracy: 0.8850 - val_loss: 4.7900 - val_accuracy: 0.4725\n\nEpoch 01941: val_loss did not improve from 2.27184\nEpoch 1942/2000\n1600/1600 [==============================] - 0s 59us/step - loss: 0.4808 - accuracy: 0.8844 - val_loss: 4.7476 - val_accuracy: 0.4600\n\nEpoch 01942: val_loss did not improve from 2.27184\nEpoch 1943/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4366 - accuracy: 0.8825 - val_loss: 4.8897 - val_accuracy: 0.4725\n\nEpoch 01943: val_loss did not improve from 2.27184\nEpoch 1944/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4896 - accuracy: 0.8875 - val_loss: 4.8662 - val_accuracy: 0.4700\n\nEpoch 01944: val_loss did not improve from 2.27184\nEpoch 1945/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.5487 - accuracy: 0.8756 - val_loss: 5.0237 - val_accuracy: 0.4725\n\nEpoch 01945: val_loss did not improve from 2.27184\nEpoch 1946/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4707 - accuracy: 0.8869 - val_loss: 5.0018 - val_accuracy: 0.4625\n\nEpoch 01946: val_loss did not improve from 2.27184\nEpoch 1947/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.4313 - accuracy: 0.8881 - val_loss: 4.8633 - val_accuracy: 0.4675\n\nEpoch 01947: val_loss did not improve from 2.27184\nEpoch 1948/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.5715 - accuracy: 0.8725 - val_loss: 5.0503 - val_accuracy: 0.4550\n\nEpoch 01948: val_loss did not improve from 2.27184\nEpoch 1949/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.5127 - accuracy: 0.8737 - val_loss: 5.0980 - val_accuracy: 0.4575\n\nEpoch 01949: val_loss did not improve from 2.27184\nEpoch 1950/2000\n1600/1600 [==============================] - 0s 59us/step - loss: 0.4329 - accuracy: 0.8869 - val_loss: 5.0145 - val_accuracy: 0.4450\n\nEpoch 01950: val_loss did not improve from 2.27184\nEpoch 1951/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4080 - accuracy: 0.8956 - val_loss: 4.8068 - val_accuracy: 0.4650\n\nEpoch 01951: val_loss did not improve from 2.27184\nEpoch 1952/2000\n1600/1600 [==============================] - 0s 54us/step - loss: 0.4663 - accuracy: 0.8800 - val_loss: 4.7450 - val_accuracy: 0.4675\n\nEpoch 01952: val_loss did not improve from 2.27184\nEpoch 1953/2000\n1600/1600 [==============================] - 0s 54us/step - loss: 0.4440 - accuracy: 0.8925 - val_loss: 4.6436 - val_accuracy: 0.4850\n\nEpoch 01953: val_loss did not improve from 2.27184\nEpoch 1954/2000\n1600/1600 [==============================] - 0s 53us/step - loss: 0.5225 - accuracy: 0.8675 - val_loss: 4.5772 - val_accuracy: 0.4700\n\nEpoch 01954: val_loss did not improve from 2.27184\nEpoch 1955/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4738 - accuracy: 0.8788 - val_loss: 4.6328 - val_accuracy: 0.4600\n\nEpoch 01955: val_loss did not improve from 2.27184\nEpoch 1956/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.5346 - accuracy: 0.8637 - val_loss: 4.6645 - val_accuracy: 0.4625\n\nEpoch 01956: val_loss did not improve from 2.27184\nEpoch 1957/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.5465 - accuracy: 0.8669 - val_loss: 4.8886 - val_accuracy: 0.4725\n\nEpoch 01957: val_loss did not improve from 2.27184\nEpoch 1958/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.4656 - accuracy: 0.8788 - val_loss: 4.6828 - val_accuracy: 0.4725\n\nEpoch 01958: val_loss did not improve from 2.27184\nEpoch 1959/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4149 - accuracy: 0.8919 - val_loss: 4.6713 - val_accuracy: 0.4825\n\nEpoch 01959: val_loss did not improve from 2.27184\nEpoch 1960/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4111 - accuracy: 0.8925 - val_loss: 4.8014 - val_accuracy: 0.4875\n\nEpoch 01960: val_loss did not improve from 2.27184\nEpoch 1961/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4470 - accuracy: 0.8831 - val_loss: 4.7825 - val_accuracy: 0.4625\n\nEpoch 01961: val_loss did not improve from 2.27184\nEpoch 1962/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4492 - accuracy: 0.8875 - val_loss: 4.6146 - val_accuracy: 0.4550\n\nEpoch 01962: val_loss did not improve from 2.27184\nEpoch 1963/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4289 - accuracy: 0.8856 - val_loss: 4.8054 - val_accuracy: 0.4600\n\nEpoch 01963: val_loss did not improve from 2.27184\nEpoch 1964/2000\n1600/1600 [==============================] - 0s 219us/step - loss: 0.5789 - accuracy: 0.8675 - val_loss: 4.7900 - val_accuracy: 0.4675\n\nEpoch 01964: val_loss did not improve from 2.27184\nEpoch 1965/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4248 - accuracy: 0.8956 - val_loss: 4.7104 - val_accuracy: 0.4850\n\nEpoch 01965: val_loss did not improve from 2.27184\nEpoch 1966/2000\n1600/1600 [==============================] - 0s 59us/step - loss: 0.4093 - accuracy: 0.8875 - val_loss: 4.9095 - val_accuracy: 0.4675\n\nEpoch 01966: val_loss did not improve from 2.27184\nEpoch 1967/2000\n1600/1600 [==============================] - 0s 70us/step - loss: 0.5633 - accuracy: 0.8700 - val_loss: 4.7928 - val_accuracy: 0.4525\n\nEpoch 01967: val_loss did not improve from 2.27184\nEpoch 1968/2000\n1600/1600 [==============================] - 0s 60us/step - loss: 0.4817 - accuracy: 0.8700 - val_loss: 4.9184 - val_accuracy: 0.4725\n\nEpoch 01968: val_loss did not improve from 2.27184\nEpoch 1969/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4926 - accuracy: 0.8788 - val_loss: 4.8891 - val_accuracy: 0.4500\n\nEpoch 01969: val_loss did not improve from 2.27184\nEpoch 1970/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4390 - accuracy: 0.8988 - val_loss: 4.9409 - val_accuracy: 0.4650\n\nEpoch 01970: val_loss did not improve from 2.27184\nEpoch 1971/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.3897 - accuracy: 0.8931 - val_loss: 4.9574 - val_accuracy: 0.4525\n\nEpoch 01971: val_loss did not improve from 2.27184\nEpoch 1972/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.4926 - accuracy: 0.8769 - val_loss: 5.0211 - val_accuracy: 0.4825\n\nEpoch 01972: val_loss did not improve from 2.27184\nEpoch 1973/2000\n1600/1600 [==============================] - 0s 62us/step - loss: 0.5627 - accuracy: 0.8719 - val_loss: 4.7880 - val_accuracy: 0.4700\n\nEpoch 01973: val_loss did not improve from 2.27184\nEpoch 1974/2000\n1600/1600 [==============================] - 0s 66us/step - loss: 0.5137 - accuracy: 0.8769 - val_loss: 4.7571 - val_accuracy: 0.4625\n\nEpoch 01974: val_loss did not improve from 2.27184\nEpoch 1975/2000\n1600/1600 [==============================] - 0s 63us/step - loss: 0.4917 - accuracy: 0.8712 - val_loss: 4.6958 - val_accuracy: 0.4675\n\nEpoch 01975: val_loss did not improve from 2.27184\nEpoch 1976/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.5445 - accuracy: 0.8763 - val_loss: 4.7234 - val_accuracy: 0.4825\n\nEpoch 01976: val_loss did not improve from 2.27184\nEpoch 1977/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4197 - accuracy: 0.8950 - val_loss: 4.7116 - val_accuracy: 0.4775\n\nEpoch 01977: val_loss did not improve from 2.27184\nEpoch 1978/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4142 - accuracy: 0.8906 - val_loss: 4.9435 - val_accuracy: 0.4650\n\nEpoch 01978: val_loss did not improve from 2.27184\nEpoch 1979/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4692 - accuracy: 0.8869 - val_loss: 4.8177 - val_accuracy: 0.4800\n\nEpoch 01979: val_loss did not improve from 2.27184\nEpoch 1980/2000\n1600/1600 [==============================] - 0s 59us/step - loss: 0.5515 - accuracy: 0.8781 - val_loss: 4.6138 - val_accuracy: 0.4500\n\nEpoch 01980: val_loss did not improve from 2.27184\nEpoch 1981/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4895 - accuracy: 0.8900 - val_loss: 4.8207 - val_accuracy: 0.4800\n\nEpoch 01981: val_loss did not improve from 2.27184\nEpoch 1982/2000\n1600/1600 [==============================] - 0s 55us/step - loss: 0.4671 - accuracy: 0.8869 - val_loss: 4.7767 - val_accuracy: 0.4675\n\nEpoch 01982: val_loss did not improve from 2.27184\nEpoch 1983/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.5596 - accuracy: 0.8719 - val_loss: 4.8307 - val_accuracy: 0.4475\n\nEpoch 01983: val_loss did not improve from 2.27184\nEpoch 1984/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.5548 - accuracy: 0.8712 - val_loss: 4.6858 - val_accuracy: 0.4700\n\nEpoch 01984: val_loss did not improve from 2.27184\nEpoch 1985/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.3704 - accuracy: 0.8919 - val_loss: 4.6179 - val_accuracy: 0.4675\n\nEpoch 01985: val_loss did not improve from 2.27184\nEpoch 1986/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4723 - accuracy: 0.8763 - val_loss: 4.7931 - val_accuracy: 0.4750\n\nEpoch 01986: val_loss did not improve from 2.27184\nEpoch 1987/2000\n1600/1600 [==============================] - 0s 57us/step - loss: 0.4745 - accuracy: 0.8806 - val_loss: 4.5908 - val_accuracy: 0.4625\n\nEpoch 01987: val_loss did not improve from 2.27184\nEpoch 1988/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.5396 - accuracy: 0.8788 - val_loss: 4.5949 - val_accuracy: 0.4675\n\nEpoch 01988: val_loss did not improve from 2.27184\nEpoch 1989/2000\n1600/1600 [==============================] - 0s 53us/step - loss: 0.4072 - accuracy: 0.9006 - val_loss: 4.5059 - val_accuracy: 0.4525\n\nEpoch 01989: val_loss did not improve from 2.27184\nEpoch 1990/2000\n1600/1600 [==============================] - 0s 58us/step - loss: 0.4797 - accuracy: 0.8875 - val_loss: 4.5115 - val_accuracy: 0.4625\n\nEpoch 01990: val_loss did not improve from 2.27184\nEpoch 1991/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.4814 - accuracy: 0.8869 - val_loss: 4.4707 - val_accuracy: 0.4700\n\nEpoch 01991: val_loss did not improve from 2.27184\nEpoch 1992/2000\n1600/1600 [==============================] - 0s 59us/step - loss: 0.3952 - accuracy: 0.8856 - val_loss: 4.4984 - val_accuracy: 0.4675\n\nEpoch 01992: val_loss did not improve from 2.27184\nEpoch 1993/2000\n1600/1600 [==============================] - 0s 60us/step - loss: 0.3935 - accuracy: 0.8950 - val_loss: 4.4446 - val_accuracy: 0.4675\n\nEpoch 01993: val_loss did not improve from 2.27184\nEpoch 1994/2000\n1600/1600 [==============================] - 0s 61us/step - loss: 0.5708 - accuracy: 0.8756 - val_loss: 4.5301 - val_accuracy: 0.4725\n\nEpoch 01994: val_loss did not improve from 2.27184\nEpoch 1995/2000\n1600/1600 [==============================] - 0s 61us/step - loss: 0.4311 - accuracy: 0.8900 - val_loss: 4.5862 - val_accuracy: 0.4625\n\nEpoch 01995: val_loss did not improve from 2.27184\nEpoch 1996/2000\n1600/1600 [==============================] - 0s 60us/step - loss: 0.5417 - accuracy: 0.8581 - val_loss: 4.6406 - val_accuracy: 0.4750\n\nEpoch 01996: val_loss did not improve from 2.27184\nEpoch 1997/2000\n1600/1600 [==============================] - 0s 61us/step - loss: 0.4682 - accuracy: 0.8794 - val_loss: 4.4405 - val_accuracy: 0.4775\n\nEpoch 01997: val_loss did not improve from 2.27184\nEpoch 1998/2000\n1600/1600 [==============================] - 0s 59us/step - loss: 0.5682 - accuracy: 0.8781 - val_loss: 4.4104 - val_accuracy: 0.4600\n\nEpoch 01998: val_loss did not improve from 2.27184\nEpoch 1999/2000\n1600/1600 [==============================] - 0s 62us/step - loss: 0.4700 - accuracy: 0.8931 - val_loss: 4.5080 - val_accuracy: 0.4675\n\nEpoch 01999: val_loss did not improve from 2.27184\nEpoch 2000/2000\n1600/1600 [==============================] - 0s 56us/step - loss: 0.5070 - accuracy: 0.8706 - val_loss: 4.4749 - val_accuracy: 0.4800\n\nEpoch 02000: val_loss did not improve from 2.27184\nTraining completed in time:  0:03:40.319766\n"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 2000\n",
    "num_batch_size = 32\n",
    "\n",
    "# save model check point to the address\n",
    "checkpointer = ModelCheckpoint(filepath='save_models/weights.best.basic_mlp.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalue training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training Accuracy:  0.9818750023841858\nTesting Accuracy:  0.47999998927116394\n"
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "The performance of this model is not satisfactory. I think maybe I need to use all features and build cnn model to train."
   ]
  }
 ]
}