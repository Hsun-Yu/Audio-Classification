{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification use CNN\n",
    "In this notebook I will use all features and train a cnn model to classify audio.\n",
    "\n",
    "## Get features\n",
    "Look the following code the code is same as `get_audio_features` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Failed to connect to remote Jupyter notebook.\r\nCheck that the Jupyter Server URI setting has a valid running server specified.\r\nhttp://114.34.48.196:8888/\r\nTypeError: request to http://114.34.48.196:8888/api/contents/?1583487069511 failed, reason: connect ECONNREFUSED 114.34.48.196:8888",
     "output_type": "error",
     "traceback": [
      "Error: Failed to connect to remote Jupyter notebook.",
      "Check that the Jupyter Server URI setting has a valid running server specified.",
      "http://114.34.48.196:8888/",
      "TypeError: request to http://114.34.48.196:8888/api/contents/?1583487069511 failed, reason: connect ECONNREFUSED 114.34.48.196:8888",
      "at /home/hsunyu/.vscode-server/extensions/ms-python.python-2020.2.64397/out/client/extension.js:1:817679",
      "at processTicksAndRejections (internal/process/task_queues.js:89:5)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_pad_len = 216\n",
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=120)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset and get all features of all audios\n",
    "1. Read the csv file\n",
    "\n",
    "loop\n",
    "\n",
    "2. get and save features of the audio of the row\n",
    "\n",
    "3. save the label\n",
    "\n",
    "Look the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Finished feature extraction from  2000  files\n"
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = './ESC-50/audio/'\n",
    "\n",
    "metadata = pd.read_csv('./ESC-50/meta/esc50.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),str(row[\"filename\"]))\n",
    "    class_label = row[\"category\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data and labels and split the dataset\n",
    "I will use sklearn.preprocessing.LabelEncoder to encode the categorical text data into model-understandable numerical data.\n",
    "Here I will use sklearn.model_selection.train_test_split to split the dataset into training and testing sets. The testing set size will be 20% and I will set a random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN model\n",
    "Look the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "a, num_rows, num_columns = X.shape\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_17 (Conv2D)           (None, 119, 215, 16)      80        \n_________________________________________________________________\nmax_pooling2d_17 (MaxPooling (None, 59, 107, 16)       0         \n_________________________________________________________________\ndropout_29 (Dropout)         (None, 59, 107, 16)       0         \n_________________________________________________________________\nconv2d_18 (Conv2D)           (None, 58, 106, 32)       2080      \n_________________________________________________________________\nmax_pooling2d_18 (MaxPooling (None, 29, 53, 32)        0         \n_________________________________________________________________\ndropout_30 (Dropout)         (None, 29, 53, 32)        0         \n_________________________________________________________________\nconv2d_19 (Conv2D)           (None, 28, 52, 64)        8256      \n_________________________________________________________________\nmax_pooling2d_19 (MaxPooling (None, 14, 26, 64)        0         \n_________________________________________________________________\ndropout_31 (Dropout)         (None, 14, 26, 64)        0         \n_________________________________________________________________\nconv2d_20 (Conv2D)           (None, 13, 25, 128)       32896     \n_________________________________________________________________\nmax_pooling2d_20 (MaxPooling (None, 6, 12, 128)        0         \n_________________________________________________________________\ndropout_32 (Dropout)         (None, 6, 12, 128)        0         \n_________________________________________________________________\nglobal_average_pooling2d_5 ( (None, 128)               0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 512)               66048     \n_________________________________________________________________\nactivation_13 (Activation)   (None, 512)               0         \n_________________________________________________________________\ndropout_33 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 512)               262656    \n_________________________________________________________________\nactivation_14 (Activation)   (None, 512)               0         \n_________________________________________________________________\ndropout_34 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 512)               262656    \n_________________________________________________________________\nactivation_15 (Activation)   (None, 512)               0         \n_________________________________________________________________\ndropout_35 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 50)                25650     \n=================================================================\nTotal params: 660,322\nTrainable params: 660,322\nNon-trainable params: 0\n_________________________________________________________________\n400/400 [==============================] - 0s 254us/step\nPre-training accuracy: 1.7500%\n"
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100 * score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "- val_accuracy: 0.6650\n\nEpoch 00902: val_loss did not improve from 1.68152\nEpoch 903/1000\n1600/1600 [==============================] - 0s 224us/step - loss: 0.0750 - accuracy: 0.9781 - val_loss: 2.0496 - val_accuracy: 0.6675\n\nEpoch 00903: val_loss did not improve from 1.68152\nEpoch 904/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0900 - accuracy: 0.9719 - val_loss: 2.0296 - val_accuracy: 0.6650\n\nEpoch 00904: val_loss did not improve from 1.68152\nEpoch 905/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0836 - accuracy: 0.9756 - val_loss: 1.9424 - val_accuracy: 0.6675\n\nEpoch 00905: val_loss did not improve from 1.68152\nEpoch 906/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0797 - accuracy: 0.9750 - val_loss: 2.0368 - val_accuracy: 0.6750\n\nEpoch 00906: val_loss did not improve from 1.68152\nEpoch 907/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1026 - accuracy: 0.9706 - val_loss: 1.8817 - val_accuracy: 0.6775\n\nEpoch 00907: val_loss did not improve from 1.68152\nEpoch 908/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0767 - accuracy: 0.9787 - val_loss: 2.0334 - val_accuracy: 0.6600\n\nEpoch 00908: val_loss did not improve from 1.68152\nEpoch 909/1000\n1600/1600 [==============================] - 0s 223us/step - loss: 0.0912 - accuracy: 0.9719 - val_loss: 2.0420 - val_accuracy: 0.6675\n\nEpoch 00909: val_loss did not improve from 1.68152\nEpoch 910/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0995 - accuracy: 0.9700 - val_loss: 2.0574 - val_accuracy: 0.6675\n\nEpoch 00910: val_loss did not improve from 1.68152\nEpoch 911/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1101 - accuracy: 0.9688 - val_loss: 2.0597 - val_accuracy: 0.6325\n\nEpoch 00911: val_loss did not improve from 1.68152\nEpoch 912/1000\n1600/1600 [==============================] - 0s 224us/step - loss: 0.0845 - accuracy: 0.9756 - val_loss: 2.0401 - val_accuracy: 0.6675\n\nEpoch 00912: val_loss did not improve from 1.68152\nEpoch 913/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1019 - accuracy: 0.9681 - val_loss: 2.2097 - val_accuracy: 0.6375\n\nEpoch 00913: val_loss did not improve from 1.68152\nEpoch 914/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.1054 - accuracy: 0.9675 - val_loss: 2.2066 - val_accuracy: 0.6550\n\nEpoch 00914: val_loss did not improve from 1.68152\nEpoch 915/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0914 - accuracy: 0.9725 - val_loss: 2.2504 - val_accuracy: 0.6550\n\nEpoch 00915: val_loss did not improve from 1.68152\nEpoch 916/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0790 - accuracy: 0.9750 - val_loss: 2.1966 - val_accuracy: 0.6475\n\nEpoch 00916: val_loss did not improve from 1.68152\nEpoch 917/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1145 - accuracy: 0.9631 - val_loss: 2.0232 - val_accuracy: 0.6650\n\nEpoch 00917: val_loss did not improve from 1.68152\nEpoch 918/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0905 - accuracy: 0.9706 - val_loss: 2.2637 - val_accuracy: 0.6525\n\nEpoch 00918: val_loss did not improve from 1.68152\nEpoch 919/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1063 - accuracy: 0.9700 - val_loss: 2.0680 - val_accuracy: 0.6625\n\nEpoch 00919: val_loss did not improve from 1.68152\nEpoch 920/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0875 - accuracy: 0.9769 - val_loss: 2.1547 - val_accuracy: 0.6550\n\nEpoch 00920: val_loss did not improve from 1.68152\nEpoch 921/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0876 - accuracy: 0.9663 - val_loss: 2.0318 - val_accuracy: 0.6725\n\nEpoch 00921: val_loss did not improve from 1.68152\nEpoch 922/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0980 - accuracy: 0.9731 - val_loss: 1.9981 - val_accuracy: 0.6525\n\nEpoch 00922: val_loss did not improve from 1.68152\nEpoch 923/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0932 - accuracy: 0.9737 - val_loss: 1.9716 - val_accuracy: 0.6700\n\nEpoch 00923: val_loss did not improve from 1.68152\nEpoch 924/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0749 - accuracy: 0.9725 - val_loss: 2.1382 - val_accuracy: 0.6650\n\nEpoch 00924: val_loss did not improve from 1.68152\nEpoch 925/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0574 - accuracy: 0.9781 - val_loss: 2.0779 - val_accuracy: 0.6725\n\nEpoch 00925: val_loss did not improve from 1.68152\nEpoch 926/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0524 - accuracy: 0.9812 - val_loss: 2.1738 - val_accuracy: 0.6800\n\nEpoch 00926: val_loss did not improve from 1.68152\nEpoch 927/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0603 - accuracy: 0.9794 - val_loss: 2.1208 - val_accuracy: 0.6650\n\nEpoch 00927: val_loss did not improve from 1.68152\nEpoch 928/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0643 - accuracy: 0.9800 - val_loss: 2.0276 - val_accuracy: 0.6700\n\nEpoch 00928: val_loss did not improve from 1.68152\nEpoch 929/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0707 - accuracy: 0.9812 - val_loss: 2.1671 - val_accuracy: 0.6675\n\nEpoch 00929: val_loss did not improve from 1.68152\nEpoch 930/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0567 - accuracy: 0.9806 - val_loss: 2.0608 - val_accuracy: 0.6725\n\nEpoch 00930: val_loss did not improve from 1.68152\nEpoch 931/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0694 - accuracy: 0.9769 - val_loss: 1.9193 - val_accuracy: 0.6800\n\nEpoch 00931: val_loss did not improve from 1.68152\nEpoch 932/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0515 - accuracy: 0.9844 - val_loss: 2.0199 - val_accuracy: 0.6725\n\nEpoch 00932: val_loss did not improve from 1.68152\nEpoch 933/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0602 - accuracy: 0.9869 - val_loss: 2.1294 - val_accuracy: 0.6650\n\nEpoch 00933: val_loss did not improve from 1.68152\nEpoch 934/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0588 - accuracy: 0.9819 - val_loss: 2.2029 - val_accuracy: 0.6700\n\nEpoch 00934: val_loss did not improve from 1.68152\nEpoch 935/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0612 - accuracy: 0.9794 - val_loss: 2.2478 - val_accuracy: 0.6575\n\nEpoch 00935: val_loss did not improve from 1.68152\nEpoch 936/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0806 - accuracy: 0.9781 - val_loss: 2.2243 - val_accuracy: 0.6600\n\nEpoch 00936: val_loss did not improve from 1.68152\nEpoch 937/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0623 - accuracy: 0.9794 - val_loss: 2.1159 - val_accuracy: 0.6750\n\nEpoch 00937: val_loss did not improve from 1.68152\nEpoch 938/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0705 - accuracy: 0.9769 - val_loss: 2.0925 - val_accuracy: 0.6725\n\nEpoch 00938: val_loss did not improve from 1.68152\nEpoch 939/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1171 - accuracy: 0.9700 - val_loss: 2.1873 - val_accuracy: 0.6650\n\nEpoch 00939: val_loss did not improve from 1.68152\nEpoch 940/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1116 - accuracy: 0.9638 - val_loss: 2.1151 - val_accuracy: 0.6875\n\nEpoch 00940: val_loss did not improve from 1.68152\nEpoch 941/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1011 - accuracy: 0.9669 - val_loss: 2.0306 - val_accuracy: 0.6750\n\nEpoch 00941: val_loss did not improve from 1.68152\nEpoch 942/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0817 - accuracy: 0.9737 - val_loss: 2.3008 - val_accuracy: 0.6400\n\nEpoch 00942: val_loss did not improve from 1.68152\nEpoch 943/1000\n1600/1600 [==============================] - 0s 223us/step - loss: 0.0693 - accuracy: 0.9744 - val_loss: 2.3040 - val_accuracy: 0.6675\n\nEpoch 00943: val_loss did not improve from 1.68152\nEpoch 944/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1231 - accuracy: 0.9631 - val_loss: 2.1189 - val_accuracy: 0.6925\n\nEpoch 00944: val_loss did not improve from 1.68152\nEpoch 945/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0788 - accuracy: 0.9737 - val_loss: 2.1661 - val_accuracy: 0.6725\n\nEpoch 00945: val_loss did not improve from 1.68152\nEpoch 946/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0854 - accuracy: 0.9737 - val_loss: 2.0336 - val_accuracy: 0.6650\n\nEpoch 00946: val_loss did not improve from 1.68152\nEpoch 947/1000\n1600/1600 [==============================] - 0s 219us/step - loss: 0.0953 - accuracy: 0.9719 - val_loss: 2.0086 - val_accuracy: 0.6700\n\nEpoch 00947: val_loss did not improve from 1.68152\nEpoch 948/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0841 - accuracy: 0.9744 - val_loss: 2.0354 - val_accuracy: 0.6825\n\nEpoch 00948: val_loss did not improve from 1.68152\nEpoch 949/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0814 - accuracy: 0.9794 - val_loss: 2.0918 - val_accuracy: 0.6700\n\nEpoch 00949: val_loss did not improve from 1.68152\nEpoch 950/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0843 - accuracy: 0.9719 - val_loss: 2.1358 - val_accuracy: 0.6800\n\nEpoch 00950: val_loss did not improve from 1.68152\nEpoch 951/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0849 - accuracy: 0.9769 - val_loss: 2.2956 - val_accuracy: 0.6575\n\nEpoch 00951: val_loss did not improve from 1.68152\nEpoch 952/1000\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1006 - accuracy: 0.9700 - val_loss: 2.3991 - val_accuracy: 0.6425\n\nEpoch 00952: val_loss did not improve from 1.68152\nEpoch 953/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1143 - accuracy: 0.9681 - val_loss: 2.1114 - val_accuracy: 0.6950\n\nEpoch 00953: val_loss did not improve from 1.68152\nEpoch 954/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1299 - accuracy: 0.9663 - val_loss: 2.2262 - val_accuracy: 0.6575\n\nEpoch 00954: val_loss did not improve from 1.68152\nEpoch 955/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0810 - accuracy: 0.9719 - val_loss: 2.1322 - val_accuracy: 0.6600\n\nEpoch 00955: val_loss did not improve from 1.68152\nEpoch 956/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1098 - accuracy: 0.9712 - val_loss: 2.1032 - val_accuracy: 0.6725\n\nEpoch 00956: val_loss did not improve from 1.68152\nEpoch 957/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.1119 - accuracy: 0.9656 - val_loss: 2.0093 - val_accuracy: 0.6725\n\nEpoch 00957: val_loss did not improve from 1.68152\nEpoch 958/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1366 - accuracy: 0.9563 - val_loss: 2.1485 - val_accuracy: 0.6475\n\nEpoch 00958: val_loss did not improve from 1.68152\nEpoch 959/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1189 - accuracy: 0.9631 - val_loss: 2.2035 - val_accuracy: 0.6525\n\nEpoch 00959: val_loss did not improve from 1.68152\nEpoch 960/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1273 - accuracy: 0.9631 - val_loss: 2.2121 - val_accuracy: 0.6450\n\nEpoch 00960: val_loss did not improve from 1.68152\nEpoch 961/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0820 - accuracy: 0.9750 - val_loss: 2.1734 - val_accuracy: 0.6600\n\nEpoch 00961: val_loss did not improve from 1.68152\nEpoch 962/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0870 - accuracy: 0.9744 - val_loss: 2.1419 - val_accuracy: 0.6725\n\nEpoch 00962: val_loss did not improve from 1.68152\nEpoch 963/1000\n1600/1600 [==============================] - 0s 224us/step - loss: 0.0984 - accuracy: 0.9688 - val_loss: 2.2385 - val_accuracy: 0.6475\n\nEpoch 00963: val_loss did not improve from 1.68152\nEpoch 964/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1004 - accuracy: 0.9719 - val_loss: 2.1735 - val_accuracy: 0.6675\n\nEpoch 00964: val_loss did not improve from 1.68152\nEpoch 965/1000\n1600/1600 [==============================] - 0s 219us/step - loss: 0.1114 - accuracy: 0.9675 - val_loss: 1.9212 - val_accuracy: 0.6900\n\nEpoch 00965: val_loss did not improve from 1.68152\nEpoch 966/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0870 - accuracy: 0.9731 - val_loss: 2.0394 - val_accuracy: 0.6775\n\nEpoch 00966: val_loss did not improve from 1.68152\nEpoch 967/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0895 - accuracy: 0.9719 - val_loss: 2.1346 - val_accuracy: 0.6650\n\nEpoch 00967: val_loss did not improve from 1.68152\nEpoch 968/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0686 - accuracy: 0.9800 - val_loss: 2.0519 - val_accuracy: 0.6750\n\nEpoch 00968: val_loss did not improve from 1.68152\nEpoch 969/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0729 - accuracy: 0.9775 - val_loss: 2.1670 - val_accuracy: 0.6650\n\nEpoch 00969: val_loss did not improve from 1.68152\nEpoch 970/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0762 - accuracy: 0.9731 - val_loss: 2.2449 - val_accuracy: 0.6650\n\nEpoch 00970: val_loss did not improve from 1.68152\nEpoch 971/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0452 - accuracy: 0.9869 - val_loss: 2.1998 - val_accuracy: 0.6775\n\nEpoch 00971: val_loss did not improve from 1.68152\nEpoch 972/1000\n1600/1600 [==============================] - 0s 226us/step - loss: 0.0535 - accuracy: 0.9806 - val_loss: 2.1761 - val_accuracy: 0.6800\n\nEpoch 00972: val_loss did not improve from 1.68152\nEpoch 973/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0540 - accuracy: 0.9844 - val_loss: 2.2486 - val_accuracy: 0.6825\n\nEpoch 00973: val_loss did not improve from 1.68152\nEpoch 974/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0680 - accuracy: 0.9794 - val_loss: 2.2164 - val_accuracy: 0.6775\n\nEpoch 00974: val_loss did not improve from 1.68152\nEpoch 975/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0665 - accuracy: 0.9750 - val_loss: 2.3298 - val_accuracy: 0.6625\n\nEpoch 00975: val_loss did not improve from 1.68152\nEpoch 976/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0488 - accuracy: 0.9806 - val_loss: 2.3190 - val_accuracy: 0.6825\n\nEpoch 00976: val_loss did not improve from 1.68152\nEpoch 977/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0921 - accuracy: 0.9737 - val_loss: 2.1946 - val_accuracy: 0.6800\n\nEpoch 00977: val_loss did not improve from 1.68152\nEpoch 978/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0621 - accuracy: 0.9781 - val_loss: 2.4931 - val_accuracy: 0.6575\n\nEpoch 00978: val_loss did not improve from 1.68152\nEpoch 979/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0685 - accuracy: 0.9812 - val_loss: 2.2946 - val_accuracy: 0.6775\n\nEpoch 00979: val_loss did not improve from 1.68152\nEpoch 980/1000\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1067 - accuracy: 0.9744 - val_loss: 2.5422 - val_accuracy: 0.6600\n\nEpoch 00980: val_loss did not improve from 1.68152\nEpoch 981/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1101 - accuracy: 0.9700 - val_loss: 2.4149 - val_accuracy: 0.6750\n\nEpoch 00981: val_loss did not improve from 1.68152\nEpoch 982/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1174 - accuracy: 0.9681 - val_loss: 2.5175 - val_accuracy: 0.6600\n\nEpoch 00982: val_loss did not improve from 1.68152\nEpoch 983/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1109 - accuracy: 0.9694 - val_loss: 2.1754 - val_accuracy: 0.6600\n\nEpoch 00983: val_loss did not improve from 1.68152\nEpoch 984/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1138 - accuracy: 0.9694 - val_loss: 2.1182 - val_accuracy: 0.6800\n\nEpoch 00984: val_loss did not improve from 1.68152\nEpoch 985/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0856 - accuracy: 0.9737 - val_loss: 2.2188 - val_accuracy: 0.6675\n\nEpoch 00985: val_loss did not improve from 1.68152\nEpoch 986/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0847 - accuracy: 0.9769 - val_loss: 2.1125 - val_accuracy: 0.6625\n\nEpoch 00986: val_loss did not improve from 1.68152\nEpoch 987/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0746 - accuracy: 0.9812 - val_loss: 2.2764 - val_accuracy: 0.6575\n\nEpoch 00987: val_loss did not improve from 1.68152\nEpoch 988/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0837 - accuracy: 0.9756 - val_loss: 2.1301 - val_accuracy: 0.6725\n\nEpoch 00988: val_loss did not improve from 1.68152\nEpoch 989/1000\n1600/1600 [==============================] - 0s 224us/step - loss: 0.0819 - accuracy: 0.9756 - val_loss: 2.2161 - val_accuracy: 0.6725\n\nEpoch 00989: val_loss did not improve from 1.68152\nEpoch 990/1000\n1600/1600 [==============================] - 0s 223us/step - loss: 0.0755 - accuracy: 0.9775 - val_loss: 2.2104 - val_accuracy: 0.6575\n\nEpoch 00990: val_loss did not improve from 1.68152\nEpoch 991/1000\n1536/1600 [===========================>..] - ETA: 0s - loss: 0.0610 - accuracy: 0.981600/1600 [==============================] - 0s 222us/step - loss: 0.0600 - accuracy: 0.9825 - val_loss: 2.1519 - val_accuracy: 0.6700\n\nEpoch 00991: val_loss did not improve from 1.68152\nEpoch 992/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0763 - accuracy: 0.9750 - val_loss: 2.1579 - val_accuracy: 0.6675\n\nEpoch 00992: val_loss did not improve from 1.68152\nEpoch 993/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0733 - accuracy: 0.9800 - val_loss: 2.2147 - val_accuracy: 0.6800\n\nEpoch 00993: val_loss did not improve from 1.68152\nEpoch 994/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0757 - accuracy: 0.9744 - val_loss: 2.1590 - val_accuracy: 0.6875\n\nEpoch 00994: val_loss did not improve from 1.68152\nEpoch 995/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0693 - accuracy: 0.9775 - val_loss: 2.1626 - val_accuracy: 0.6700\n\nEpoch 00995: val_loss did not improve from 1.68152\nEpoch 996/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0714 - accuracy: 0.9781 - val_loss: 2.1661 - val_accuracy: 0.6700\n\nEpoch 00996: val_loss did not improve from 1.68152\nEpoch 997/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0697 - accuracy: 0.9737 - val_loss: 2.1635 - val_accuracy: 0.6675\n\nEpoch 00997: val_loss did not improve from 1.68152\nEpoch 998/1000\n1600/1600 [==============================] - 0s 221us/step - loss: 0.0934 - accuracy: 0.9775 - val_loss: 2.2014 - val_accuracy: 0.6600\n\nEpoch 00998: val_loss did not improve from 1.68152\nEpoch 999/1000\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0610 - accuracy: 0.9819 - val_loss: 2.2688 - val_accuracy: 0.6775\n\nEpoch 00999: val_loss did not improve from 1.68152\nEpoch 1000/1000\n1600/1600 [==============================] - 0s 220us/step - loss: 0.0624 - accuracy: 0.9812 - val_loss: 2.4200 - val_accuracy: 0.6700\n\nEpoch 01000: val_loss did not improve from 1.68152\nTraining completed in time:  0:05:58.842616\n"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 1000\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='save_models/weights.best.basic_cnn10.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalue training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training Accuracy:  0.9900000095367432\nTesting Accuracy:  0.6700000166893005\n"
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "The accuracy from (DNN version)0.48 to (CNN version)0.67"
   ]
  }
 ]
}