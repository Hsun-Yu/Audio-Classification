{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification use CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_pad_len = 216\n",
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=120)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Finished feature extraction from  2000  files\n"
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = '../ESC-50/audio/'\n",
    "\n",
    "metadata = pd.read_csv('../ESC-50/meta/esc50.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),str(row[\"filename\"]))\n",
    "    class_label = row[\"category\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "a, num_rows, num_columns = X.shape\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_24\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_87 (Conv2D)           (None, 119, 215, 16)      80        \n_________________________________________________________________\nmax_pooling2d_85 (MaxPooling (None, 59, 107, 16)       0         \n_________________________________________________________________\ndropout_133 (Dropout)        (None, 59, 107, 16)       0         \n_________________________________________________________________\nconv2d_88 (Conv2D)           (None, 58, 106, 32)       2080      \n_________________________________________________________________\nmax_pooling2d_86 (MaxPooling (None, 29, 53, 32)        0         \n_________________________________________________________________\ndropout_134 (Dropout)        (None, 29, 53, 32)        0         \n_________________________________________________________________\nconv2d_89 (Conv2D)           (None, 28, 52, 64)        8256      \n_________________________________________________________________\nmax_pooling2d_87 (MaxPooling (None, 14, 26, 64)        0         \n_________________________________________________________________\ndropout_135 (Dropout)        (None, 14, 26, 64)        0         \n_________________________________________________________________\nconv2d_90 (Conv2D)           (None, 13, 25, 128)       32896     \n_________________________________________________________________\nmax_pooling2d_88 (MaxPooling (None, 6, 12, 128)        0         \n_________________________________________________________________\ndropout_136 (Dropout)        (None, 6, 12, 128)        0         \n_________________________________________________________________\nglobal_average_pooling2d_22  (None, 128)               0         \n_________________________________________________________________\ndense_70 (Dense)             (None, 512)               66048     \n_________________________________________________________________\nactivation_49 (Activation)   (None, 512)               0         \n_________________________________________________________________\ndropout_137 (Dropout)        (None, 512)               0         \n_________________________________________________________________\ndense_71 (Dense)             (None, 1024)              525312    \n_________________________________________________________________\nactivation_50 (Activation)   (None, 1024)              0         \n_________________________________________________________________\ndropout_138 (Dropout)        (None, 1024)              0         \n_________________________________________________________________\ndense_72 (Dense)             (None, 512)               524800    \n_________________________________________________________________\nactivation_51 (Activation)   (None, 512)               0         \n_________________________________________________________________\ndropout_139 (Dropout)        (None, 512)               0         \n_________________________________________________________________\ndense_73 (Dense)             (None, 50)                25650     \n=================================================================\nTotal params: 1,185,122\nTrainable params: 1,185,122\nNon-trainable params: 0\n_________________________________________________________________\n400/400 [==============================] - 0s 247us/step\nPre-training accuracy: 2.0000%\n"
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100 * score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6400\n\nEpoch 00501: val_loss did not improve from 1.58838\nEpoch 502/600\n1600/1600 [==============================] - 0s 224us/step - loss: 0.1417 - accuracy: 0.9594 - val_loss: 1.9523 - val_accuracy: 0.6475\n\nEpoch 00502: val_loss did not improve from 1.58838\nEpoch 503/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1438 - accuracy: 0.9513 - val_loss: 1.8777 - val_accuracy: 0.6575\n\nEpoch 00503: val_loss did not improve from 1.58838\nEpoch 504/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1459 - accuracy: 0.9569 - val_loss: 1.9346 - val_accuracy: 0.6425\n\nEpoch 00504: val_loss did not improve from 1.58838\nEpoch 505/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1546 - accuracy: 0.9456 - val_loss: 2.0498 - val_accuracy: 0.6250\n\nEpoch 00505: val_loss did not improve from 1.58838\nEpoch 506/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1483 - accuracy: 0.9569 - val_loss: 1.9523 - val_accuracy: 0.6525\n\nEpoch 00506: val_loss did not improve from 1.58838\nEpoch 507/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1769 - accuracy: 0.9494 - val_loss: 1.9580 - val_accuracy: 0.6375\n\nEpoch 00507: val_loss did not improve from 1.58838\nEpoch 508/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1622 - accuracy: 0.9444 - val_loss: 1.8880 - val_accuracy: 0.6525\n\nEpoch 00508: val_loss did not improve from 1.58838\nEpoch 509/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1645 - accuracy: 0.9506 - val_loss: 1.8512 - val_accuracy: 0.6525\n\nEpoch 00509: val_loss did not improve from 1.58838\nEpoch 510/600\n1600/1600 [==============================] - 0s 224us/step - loss: 0.1567 - accuracy: 0.9494 - val_loss: 1.8896 - val_accuracy: 0.6425\n\nEpoch 00510: val_loss did not improve from 1.58838\nEpoch 511/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1444 - accuracy: 0.9600 - val_loss: 1.9793 - val_accuracy: 0.6350\n\nEpoch 00511: val_loss did not improve from 1.58838\nEpoch 512/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1509 - accuracy: 0.9506 - val_loss: 2.0153 - val_accuracy: 0.6200\n\nEpoch 00512: val_loss did not improve from 1.58838\nEpoch 513/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1593 - accuracy: 0.9500 - val_loss: 1.7780 - val_accuracy: 0.6600\n\nEpoch 00513: val_loss did not improve from 1.58838\nEpoch 514/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1560 - accuracy: 0.9506 - val_loss: 1.9424 - val_accuracy: 0.6425\n\nEpoch 00514: val_loss did not improve from 1.58838\nEpoch 515/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1546 - accuracy: 0.9525 - val_loss: 1.9000 - val_accuracy: 0.6500\n\nEpoch 00515: val_loss did not improve from 1.58838\nEpoch 516/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1332 - accuracy: 0.9581 - val_loss: 1.9644 - val_accuracy: 0.6625\n\nEpoch 00516: val_loss did not improve from 1.58838\nEpoch 517/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1431 - accuracy: 0.9550 - val_loss: 1.8788 - val_accuracy: 0.6475\n\nEpoch 00517: val_loss did not improve from 1.58838\nEpoch 518/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1594 - accuracy: 0.9481 - val_loss: 2.0445 - val_accuracy: 0.6350\n\nEpoch 00518: val_loss did not improve from 1.58838\nEpoch 519/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1487 - accuracy: 0.9563 - val_loss: 1.9160 - val_accuracy: 0.6425\n\nEpoch 00519: val_loss did not improve from 1.58838\nEpoch 520/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1384 - accuracy: 0.9606 - val_loss: 1.9614 - val_accuracy: 0.6475\n\nEpoch 00520: val_loss did not improve from 1.58838\nEpoch 521/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1376 - accuracy: 0.9531 - val_loss: 1.8201 - val_accuracy: 0.6625\n\nEpoch 00521: val_loss did not improve from 1.58838\nEpoch 522/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1217 - accuracy: 0.9581 - val_loss: 1.8637 - val_accuracy: 0.6425\n\nEpoch 00522: val_loss did not improve from 1.58838\nEpoch 523/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1161 - accuracy: 0.9606 - val_loss: 1.9285 - val_accuracy: 0.6700\n\nEpoch 00523: val_loss did not improve from 1.58838\nEpoch 524/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1190 - accuracy: 0.9644 - val_loss: 1.8098 - val_accuracy: 0.6675\n\nEpoch 00524: val_loss did not improve from 1.58838\nEpoch 525/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1012 - accuracy: 0.9644 - val_loss: 1.9257 - val_accuracy: 0.6500\n\nEpoch 00525: val_loss did not improve from 1.58838\nEpoch 526/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1199 - accuracy: 0.9619 - val_loss: 1.7941 - val_accuracy: 0.6675\n\nEpoch 00526: val_loss did not improve from 1.58838\nEpoch 527/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1175 - accuracy: 0.9600 - val_loss: 1.9287 - val_accuracy: 0.6375\n\nEpoch 00527: val_loss did not improve from 1.58838\nEpoch 528/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1160 - accuracy: 0.9631 - val_loss: 2.0040 - val_accuracy: 0.6375\n\nEpoch 00528: val_loss did not improve from 1.58838\nEpoch 529/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1196 - accuracy: 0.9644 - val_loss: 2.0500 - val_accuracy: 0.6600\n\nEpoch 00529: val_loss did not improve from 1.58838\nEpoch 530/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1191 - accuracy: 0.9569 - val_loss: 1.9202 - val_accuracy: 0.6475\n\nEpoch 00530: val_loss did not improve from 1.58838\nEpoch 531/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1137 - accuracy: 0.9594 - val_loss: 2.0377 - val_accuracy: 0.6275\n\nEpoch 00531: val_loss did not improve from 1.58838\nEpoch 532/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1144 - accuracy: 0.9631 - val_loss: 1.9536 - val_accuracy: 0.6400\n\nEpoch 00532: val_loss did not improve from 1.58838\nEpoch 533/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1544 - accuracy: 0.9506 - val_loss: 1.9996 - val_accuracy: 0.6350\n\nEpoch 00533: val_loss did not improve from 1.58838\nEpoch 534/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1216 - accuracy: 0.9631 - val_loss: 1.9702 - val_accuracy: 0.6400\n\nEpoch 00534: val_loss did not improve from 1.58838\nEpoch 535/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1412 - accuracy: 0.9563 - val_loss: 1.9743 - val_accuracy: 0.6600\n\nEpoch 00535: val_loss did not improve from 1.58838\nEpoch 536/600\n1600/1600 [==============================] - 0s 224us/step - loss: 0.1252 - accuracy: 0.9563 - val_loss: 2.0646 - val_accuracy: 0.6350\n\nEpoch 00536: val_loss did not improve from 1.58838\nEpoch 537/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1156 - accuracy: 0.9556 - val_loss: 1.9306 - val_accuracy: 0.6700\n\nEpoch 00537: val_loss did not improve from 1.58838\nEpoch 538/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1287 - accuracy: 0.9500 - val_loss: 2.0973 - val_accuracy: 0.6225\n\nEpoch 00538: val_loss did not improve from 1.58838\nEpoch 539/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1326 - accuracy: 0.9606 - val_loss: 2.0606 - val_accuracy: 0.6350\n\nEpoch 00539: val_loss did not improve from 1.58838\nEpoch 540/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 2.0400 - val_accuracy: 0.6375\n\nEpoch 00540: val_loss did not improve from 1.58838\nEpoch 541/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1313 - accuracy: 0.9550 - val_loss: 2.0007 - val_accuracy: 0.6550\n\nEpoch 00541: val_loss did not improve from 1.58838\nEpoch 542/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1250 - accuracy: 0.9556 - val_loss: 1.9736 - val_accuracy: 0.6575\n\nEpoch 00542: val_loss did not improve from 1.58838\nEpoch 543/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1306 - accuracy: 0.9569 - val_loss: 1.9808 - val_accuracy: 0.6400\n\nEpoch 00543: val_loss did not improve from 1.58838\nEpoch 544/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1343 - accuracy: 0.9556 - val_loss: 1.9927 - val_accuracy: 0.6525\n\nEpoch 00544: val_loss did not improve from 1.58838\nEpoch 545/600\n1600/1600 [==============================] - 0s 227us/step - loss: 0.1153 - accuracy: 0.9619 - val_loss: 2.0111 - val_accuracy: 0.6500\n\nEpoch 00545: val_loss did not improve from 1.58838\nEpoch 546/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1306 - accuracy: 0.9569 - val_loss: 2.0060 - val_accuracy: 0.6475\n\nEpoch 00546: val_loss did not improve from 1.58838\nEpoch 547/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1334 - accuracy: 0.9575 - val_loss: 1.9381 - val_accuracy: 0.6575\n\nEpoch 00547: val_loss did not improve from 1.58838\nEpoch 548/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1350 - accuracy: 0.9563 - val_loss: 2.1135 - val_accuracy: 0.6350\n\nEpoch 00548: val_loss did not improve from 1.58838\nEpoch 549/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1380 - accuracy: 0.9531 - val_loss: 2.0063 - val_accuracy: 0.6500\n\nEpoch 00549: val_loss did not improve from 1.58838\nEpoch 550/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1340 - accuracy: 0.9563 - val_loss: 1.9176 - val_accuracy: 0.6575\n\nEpoch 00550: val_loss did not improve from 1.58838\nEpoch 551/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1683 - accuracy: 0.9494 - val_loss: 1.9976 - val_accuracy: 0.6200\n\nEpoch 00551: val_loss did not improve from 1.58838\nEpoch 552/600\n1600/1600 [==============================] - 0s 224us/step - loss: 0.1813 - accuracy: 0.9400 - val_loss: 1.9488 - val_accuracy: 0.6375\n\nEpoch 00552: val_loss did not improve from 1.58838\nEpoch 553/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1545 - accuracy: 0.9563 - val_loss: 1.8384 - val_accuracy: 0.6500\n\nEpoch 00553: val_loss did not improve from 1.58838\nEpoch 554/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1346 - accuracy: 0.9588 - val_loss: 1.8601 - val_accuracy: 0.6325\n\nEpoch 00554: val_loss did not improve from 1.58838\nEpoch 555/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1296 - accuracy: 0.9594 - val_loss: 1.9383 - val_accuracy: 0.6400\n\nEpoch 00555: val_loss did not improve from 1.58838\nEpoch 556/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1212 - accuracy: 0.9588 - val_loss: 1.9671 - val_accuracy: 0.6350\n\nEpoch 00556: val_loss did not improve from 1.58838\nEpoch 557/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1546 - accuracy: 0.9525 - val_loss: 1.9308 - val_accuracy: 0.6375\n\nEpoch 00557: val_loss did not improve from 1.58838\nEpoch 558/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1474 - accuracy: 0.9513 - val_loss: 2.0047 - val_accuracy: 0.6300\n\nEpoch 00558: val_loss did not improve from 1.58838\nEpoch 559/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1301 - accuracy: 0.9600 - val_loss: 1.8548 - val_accuracy: 0.6350\n\nEpoch 00559: val_loss did not improve from 1.58838\nEpoch 560/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1568 - accuracy: 0.9569 - val_loss: 1.8756 - val_accuracy: 0.6475\n\nEpoch 00560: val_loss did not improve from 1.58838\nEpoch 561/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1290 - accuracy: 0.9638 - val_loss: 1.9787 - val_accuracy: 0.6300\n\nEpoch 00561: val_loss did not improve from 1.58838\nEpoch 562/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1650 - accuracy: 0.9481 - val_loss: 1.9507 - val_accuracy: 0.6550\n\nEpoch 00562: val_loss did not improve from 1.58838\nEpoch 563/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1676 - accuracy: 0.9463 - val_loss: 1.9384 - val_accuracy: 0.6175\n\nEpoch 00563: val_loss did not improve from 1.58838\nEpoch 564/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1447 - accuracy: 0.9475 - val_loss: 1.9566 - val_accuracy: 0.6225\n\nEpoch 00564: val_loss did not improve from 1.58838\nEpoch 565/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1366 - accuracy: 0.9594 - val_loss: 1.8239 - val_accuracy: 0.6375\n\nEpoch 00565: val_loss did not improve from 1.58838\nEpoch 566/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1349 - accuracy: 0.9588 - val_loss: 1.7891 - val_accuracy: 0.6600\n\nEpoch 00566: val_loss did not improve from 1.58838\nEpoch 567/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1335 - accuracy: 0.9675 - val_loss: 1.9093 - val_accuracy: 0.6425\n\nEpoch 00567: val_loss did not improve from 1.58838\nEpoch 568/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1432 - accuracy: 0.9550 - val_loss: 1.8449 - val_accuracy: 0.6375\n\nEpoch 00568: val_loss did not improve from 1.58838\nEpoch 569/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1230 - accuracy: 0.9638 - val_loss: 1.7644 - val_accuracy: 0.6550\n\nEpoch 00569: val_loss did not improve from 1.58838\nEpoch 570/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1063 - accuracy: 0.9650 - val_loss: 1.9526 - val_accuracy: 0.6675\n\nEpoch 00570: val_loss did not improve from 1.58838\nEpoch 571/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1223 - accuracy: 0.9600 - val_loss: 1.9640 - val_accuracy: 0.6300\n\nEpoch 00571: val_loss did not improve from 1.58838\nEpoch 572/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1322 - accuracy: 0.9563 - val_loss: 2.0303 - val_accuracy: 0.6475\n\nEpoch 00572: val_loss did not improve from 1.58838\nEpoch 573/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1281 - accuracy: 0.9600 - val_loss: 1.8299 - val_accuracy: 0.6600\n\nEpoch 00573: val_loss did not improve from 1.58838\nEpoch 574/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1290 - accuracy: 0.9563 - val_loss: 1.9886 - val_accuracy: 0.6500\n\nEpoch 00574: val_loss did not improve from 1.58838\nEpoch 575/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1569 - accuracy: 0.9513 - val_loss: 1.7996 - val_accuracy: 0.6525\n\nEpoch 00575: val_loss did not improve from 1.58838\nEpoch 576/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1087 - accuracy: 0.9663 - val_loss: 1.9562 - val_accuracy: 0.6500\n\nEpoch 00576: val_loss did not improve from 1.58838\nEpoch 577/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1404 - accuracy: 0.9575 - val_loss: 1.8831 - val_accuracy: 0.6550\n\nEpoch 00577: val_loss did not improve from 1.58838\nEpoch 578/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 1.9992 - val_accuracy: 0.6325\n\nEpoch 00578: val_loss did not improve from 1.58838\nEpoch 579/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1314 - accuracy: 0.9575 - val_loss: 2.0348 - val_accuracy: 0.6200\n\nEpoch 00579: val_loss did not improve from 1.58838\nEpoch 580/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1390 - accuracy: 0.9569 - val_loss: 1.9363 - val_accuracy: 0.6550\n\nEpoch 00580: val_loss did not improve from 1.58838\nEpoch 581/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1060 - accuracy: 0.9663 - val_loss: 1.9713 - val_accuracy: 0.6550\n\nEpoch 00581: val_loss did not improve from 1.58838\nEpoch 582/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1361 - accuracy: 0.9650 - val_loss: 1.9634 - val_accuracy: 0.6500\n\nEpoch 00582: val_loss did not improve from 1.58838\nEpoch 583/600\n1600/1600 [==============================] - 0s 223us/step - loss: 0.1414 - accuracy: 0.9581 - val_loss: 1.8273 - val_accuracy: 0.6625\n\nEpoch 00583: val_loss did not improve from 1.58838\nEpoch 584/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1082 - accuracy: 0.9656 - val_loss: 1.8547 - val_accuracy: 0.6450\n\nEpoch 00584: val_loss did not improve from 1.58838\nEpoch 585/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1422 - accuracy: 0.9575 - val_loss: 1.8430 - val_accuracy: 0.6625\n\nEpoch 00585: val_loss did not improve from 1.58838\nEpoch 586/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1359 - accuracy: 0.9525 - val_loss: 1.8630 - val_accuracy: 0.6550\n\nEpoch 00586: val_loss did not improve from 1.58838\nEpoch 587/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1327 - accuracy: 0.9638 - val_loss: 1.9001 - val_accuracy: 0.6375\n\nEpoch 00587: val_loss did not improve from 1.58838\nEpoch 588/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1201 - accuracy: 0.9613 - val_loss: 1.8595 - val_accuracy: 0.6550\n\nEpoch 00588: val_loss did not improve from 1.58838\nEpoch 589/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1081 - accuracy: 0.9712 - val_loss: 1.8251 - val_accuracy: 0.6600\n\nEpoch 00589: val_loss did not improve from 1.58838\nEpoch 590/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1302 - accuracy: 0.9588 - val_loss: 1.9127 - val_accuracy: 0.6700\n\nEpoch 00590: val_loss did not improve from 1.58838\nEpoch 591/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1239 - accuracy: 0.9594 - val_loss: 1.9106 - val_accuracy: 0.6600\n\nEpoch 00591: val_loss did not improve from 1.58838\nEpoch 592/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1424 - accuracy: 0.9588 - val_loss: 2.0077 - val_accuracy: 0.6375\n\nEpoch 00592: val_loss did not improve from 1.58838\nEpoch 593/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1388 - accuracy: 0.9556 - val_loss: 1.8827 - val_accuracy: 0.6500\n\nEpoch 00593: val_loss did not improve from 1.58838\nEpoch 594/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1456 - accuracy: 0.9513 - val_loss: 1.9090 - val_accuracy: 0.6725\n\nEpoch 00594: val_loss did not improve from 1.58838\nEpoch 595/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1114 - accuracy: 0.9694 - val_loss: 1.9352 - val_accuracy: 0.6475\n\nEpoch 00595: val_loss did not improve from 1.58838\nEpoch 596/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0930 - accuracy: 0.9706 - val_loss: 1.9634 - val_accuracy: 0.6575\n\nEpoch 00596: val_loss did not improve from 1.58838\nEpoch 597/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.0920 - accuracy: 0.9737 - val_loss: 1.8957 - val_accuracy: 0.6675\n\nEpoch 00597: val_loss did not improve from 1.58838\nEpoch 598/600\n1600/1600 [==============================] - 0s 221us/step - loss: 0.1076 - accuracy: 0.9656 - val_loss: 1.9115 - val_accuracy: 0.6775\n\nEpoch 00598: val_loss did not improve from 1.58838\nEpoch 599/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1271 - accuracy: 0.9631 - val_loss: 1.9180 - val_accuracy: 0.6650\n\nEpoch 00599: val_loss did not improve from 1.58838\nEpoch 600/600\n1600/1600 [==============================] - 0s 222us/step - loss: 0.1028 - accuracy: 0.9712 - val_loss: 1.8864 - val_accuracy: 0.6600\n\nEpoch 00600: val_loss did not improve from 1.58838\nTraining completed in time:  0:03:37.633380\n"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 600\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='save_models/weights.best.basic_cnn10.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training Accuracy:  0.9950000047683716\nTesting Accuracy:  0.6600000262260437\n"
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}